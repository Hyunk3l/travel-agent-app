# Eval SOP setup

This repo already includes basic evaluation scripts in `scripts/evaluate_agents.py`
and `scripts/llm_judge.py`. The Eval SOP workflow adds a structured Plan → Data →
Eval → Report process using the Strands Evals SDK.

## Install Eval SOP tooling

```bash
pip install strands-agents-sops
```

## Prepare the SOP file

Copy the `eval.sop.md` file from the installed package into a local folder:

```bash
python - <<'PY'
from importlib import resources
import shutil
from pathlib import Path

target = Path("eval/sops")
target.mkdir(parents=True, exist_ok=True)

with resources.as_file(resources.files("strands_agents_sops") / "eval.sop.md") as src:
    shutil.copy(src, target / "eval.sop.md")
    print(f"Copied {src} -> {target / 'eval.sop.md'}")
PY
```

## Run Eval SOP (MCP recommended)

Start the MCP server:

```bash
strands-agents-sops mcp --sop-paths eval/sops
```

Then in your AI assistant (connected to MCP), run:

```
/Eval generate an evaluation plan for this agent at ./src using test cases in ./tests/eval/test_cases.json
```

Follow the prompts to generate test data, run evaluations, and produce a report.
Expected outputs (generated by the SOP) are:

- `eval/eval-plan.md`
- `eval/test-cases.jsonl`
- `eval/run_evaluation.py`
- `eval/results/`
- `eval/eval-report.md`

## Optional: convert existing test cases to JSONL

```bash
python scripts/convert_eval_cases.py
```
